{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from urllib import request\n",
        "import json\n",
        "import pandas as pd\n",
        "from math import floor, ceil, log10\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import T5Tokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "import random"
      ],
      "metadata": {
        "id": "KfWzM8SqCDwx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seed setting\n",
        "\n",
        "seed = 28\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "eL7dl_OtUY9J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3N_BS4_IOPFb"
      },
      "outputs": [],
      "source": [
        "def parse_conllu_using_pandas(block):\n",
        "    records = []\n",
        "    for line in block.splitlines():\n",
        "        if not line.startswith('#'):\n",
        "            records.append(line.strip().split('\\t'))\n",
        "    return pd.DataFrame.from_records(\n",
        "        records,\n",
        "        columns=['ID', 'FORM', 'TAG', 'Misc1', 'Misc2'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_labels(df):\n",
        "    return (\n",
        "        df.FORM.tolist(),\n",
        "        df.TAG.tolist()\n",
        "    )"
      ],
      "metadata": {
        "id": "aZgOEbOTOal8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREFIX = \"https://raw.githubusercontent.com/UniversalNER/\"\n",
        "DATA_URLS = {\n",
        "    \"en_ewt\": {\n",
        "        \"train\": \"UNER_English-EWT/master/en_ewt-ud-train.iob2\",\n",
        "        \"dev\": \"UNER_English-EWT/master/en_ewt-ud-dev.iob2\",\n",
        "        \"test\": \"UNER_English-EWT/master/en_ewt-ud-test.iob2\"\n",
        "    },\n",
        "    \"en_pud\": {\n",
        "        \"test\": \"UNER_English-PUD/master/en_pud-ud-test.iob2\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "DD0C5WOEObCZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en_ewt is the main train-dev-test split\n",
        "# en_pud is the OOD test set\n",
        "data_dict = defaultdict(dict)\n",
        "for corpus, split_dict in DATA_URLS.items():\n",
        "    for split, url_suffix in split_dict.items():\n",
        "        url = PREFIX + url_suffix\n",
        "        with request.urlopen(url) as response:\n",
        "            txt = response.read().decode('utf-8')\n",
        "            data_frames = map(parse_conllu_using_pandas,\n",
        "                              txt.strip().split('\\n\\n'))\n",
        "            token_label_alignments = list(map(tokens_to_labels,\n",
        "                                              data_frames))\n",
        "            data_dict[corpus][split] = token_label_alignments\n"
      ],
      "metadata": {
        "id": "89XxOb7yOcXT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the data so that you don't have to redownload it each time.\n",
        "with open('ner_data_dict.json', 'w', encoding='utf-8') as out:\n",
        "    json.dump(data_dict, out, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "GTD3kMtYOdix"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each subset of each corpus is a list of tuples where each tuple\n",
        "# is a list of tokens with a corresponding list of labels.\n",
        "\n",
        "# Train on data_dict['en_ewt']['train']; validate on data_dict['en_ewt']['dev']\n",
        "# and test on data_dict['en_ewt']['test'] and data_dict['en_pud']['test']\n",
        "data_dict['en_ewt']['train'][0], data_dict['en_pud']['test'][1]"
      ],
      "metadata": {
        "id": "QqXeuUIUOey2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating functions\n",
        "\n",
        "#Function to change 7 labels to simple 3 label system.\n",
        "def simplify_tags_to_3labels(tag_seq):\n",
        "  simplified_tags = []\n",
        "\n",
        "  for tag in tag_seq:\n",
        "\n",
        "    if tag == \"O\":\n",
        "      simplified_tags.append(\"O\") #Non-entities\n",
        "\n",
        "    elif tag == \"B-LOC\" or tag == \"B-PER\" or tag == \"B-ORG\": #All B tags\n",
        "      simplified_tags.append(\"B\")\n",
        "\n",
        "    elif tag == \"I-LOC\" or tag == \"I-PER\" or tag == \"I-ORG\": #All I tags\n",
        "      simplified_tags.append(\"I\")\n",
        "\n",
        "    else:\n",
        "      simplified_tags.append(\"O\") #Incase there's unexpected tags\n",
        "\n",
        "  return simplified_tags\n",
        "\n",
        "\n",
        "#function to convert the token/tag pairs in dictionaries into spaced out strings\n",
        "#also can do label simplification is simplified = true\n",
        "def prepare_data(data_tuples, simplified = False):\n",
        "  word_inputs = [] #list of token strings\n",
        "  label_outputs = [] #list of label strings\n",
        "\n",
        "  for tokens, tags in data_tuples:\n",
        "    if simplified:\n",
        "      tags = simplify_tags_to_3labels(tags) #if simple tagset\n",
        "\n",
        "    word_inputs.append(\" \".join(tokens)) #join tokens to string with a space\n",
        "    label_outputs.append(\" \".join(tags))\n",
        "\n",
        "\n",
        "  return word_inputs, label_outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "TLQFdb1G9qTv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA PREPARATION\n",
        "\n",
        "#COnvert the train, dev and test splits into token and label strings\n",
        "prep_train_sentences, prep_train_tags = prepare_data(data_dict[\"en_ewt\"][\"train\"], simplified = False)\n",
        "\n",
        "prep_dev_sentences, prep_dev_tags = prepare_data(data_dict[\"en_ewt\"][\"dev\"], simplified = False)\n",
        "\n",
        "prep_test_sentences1, prep_test_labels1 = (prepare_data(data_dict[\"en_ewt\"][\"test\"], simplified=False))\n",
        "\n",
        "prep_test_sentences2, prep_test_labels2 = (prepare_data(data_dict[\"en_pud\"][\"test\"], simplified=False))\n",
        "\n",
        "#Merges the test sets\n",
        "prep_test_sentences = prep_test_sentences1 + prep_test_sentences2\n",
        "prep_test_labels = prep_test_labels2 + prep_test_labels1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TOKENISING THE DATA\n",
        "\n",
        "#Load T5 tokeniser\n",
        "tokeniser = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "#convert strings to token IDs, apply padding/truncation up 128 tokens\n",
        "tokenised_train_sentences = tokeniser(prep_train_sentences, padding=\"max_length\", truncation = True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "tokenised_train_labels = tokeniser(prep_train_tags, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "tokenised_dev_sentences = tokeniser(prep_dev_sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "tokenised_dev_labels = tokeniser(prep_dev_tags, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "tokenised_test_sentences = tokeniser(prep_test_sentences, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "tokenised_test_labels = tokeniser(prep_test_labels, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#PREFIXING INPUT\n",
        "\n",
        "\n",
        "#Tell t5 that the task is NER to guide it\n",
        "prep_train_sentences = [\"ner: \" + s for s in prep_train_sentences]\n",
        "prep_dev_sentences   = [\"ner: \" + s for s in prep_dev_sentences]\n",
        "prep_test_sentences  = [\"ner: \" + s for s in prep_test_sentences]\n"
      ],
      "metadata": {
        "id": "qtFMu8j__u-n"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change padding to -100 in the labels\n",
        "\n",
        "\n",
        "#clone the original input so can modify it but keep the original\n",
        "train_labels = tokenised_train_labels.input_ids.clone()\n",
        "\n",
        "train_size = train_labels.shape[0]\n",
        "train_sequen_length = train_labels.shape[1]\n",
        "\n",
        "for i in range(train_size):\n",
        "  for j in range(train_sequen_length):\n",
        "\n",
        "    if train_labels[i, j].item() == tokeniser.pad_token_id: #replace the padding token\n",
        "\n",
        "      train_labels[i, j] =-100\n",
        "\n",
        "\n",
        "#repeat the same procedure for the dev set\n",
        "dev_labels = tokenised_dev_labels.input_ids.clone()\n",
        "\n",
        "dev_size = dev_labels.shape[0]\n",
        "dev_sequen_length = dev_labels.shape[1]\n",
        "\n",
        "for i in range(dev_size):\n",
        "  for j in range(dev_sequen_length):\n",
        "\n",
        "    if dev_labels[i, j].item() == tokeniser.pad_token_id:\n",
        "\n",
        "      dev_labels[i,j] = -100\n",
        "\n"
      ],
      "metadata": {
        "id": "edUrP4UDI07Q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make tensor dataset because the model expects batches of (input_ids, attention_mask, labels) in training & validation\n",
        "\n",
        "training_dataset = TensorDataset(tokenised_train_sentences.input_ids, tokenised_train_sentences.attention_mask, train_labels)\n",
        "\n",
        "dev_dataset = TensorDataset(tokenised_dev_sentences.input_ids, tokenised_dev_sentences.attention_mask, dev_labels)"
      ],
      "metadata": {
        "id": "LSOm5n0EKWjL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the device as cuda\n",
        "\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "cLN9pYr-RNJL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Get the actual T5 model and move it to cuda\n",
        "\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fGYHloVCQvpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffling data before batching to the model overfitting to one sequence of data\n",
        "\n",
        "input_ids = tokenised_train_sentences.input_ids\n",
        "attention_mask = tokenised_train_sentences.attention_mask\n",
        "\n",
        "N = input_ids.shape[0]\n",
        "\n",
        "#generating random permutation for shuffling\n",
        "randperm = torch.randperm(N)\n",
        "\n",
        "#apply the random permutation to data\n",
        "shuffled_input_ids = input_ids[randperm]\n",
        "shuffled_attention_mask = attention_mask[randperm]\n",
        "shuffled_labels = train_labels[randperm]\n",
        "\n",
        "#move data to cuda\n",
        "shuffled_input_ids = shuffled_input_ids.to(device)\n",
        "shuffled_attention_mask = shuffled_attention_mask.to(device)\n",
        "shuffled_labels = shuffled_labels.to(device)\n"
      ],
      "metadata": {
        "id": "LZ9baKAQRHBs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING LOOP\n",
        "\n",
        "#define optimiser\n",
        "optimiser = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "num_steps = ceil(N/batch_size)\n",
        "\n",
        "#loop over epochs\n",
        "for epoch in range(1, epochs+1):\n",
        "  model.train() #set model to training\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "\n",
        "  for step in range(num_steps):\n",
        "\n",
        "    #loop over batches\n",
        "    low = step * batch_size\n",
        "    high = low + batch_size\n",
        "\n",
        "    batch_input_ids = shuffled_input_ids[low:high].to(device)\n",
        "    batch_attention_mask = shuffled_attention_mask[low:high].to(device)\n",
        "    batch_labels = shuffled_labels[low:high].to(device)\n",
        "\n",
        "    #Forward pass - predicting outputs and loss\n",
        "    outputs = model(input_ids=batch_input_ids, attention_mask = batch_attention_mask, labels = batch_labels)\n",
        "\n",
        "    loss = outputs.loss # cross entropy loss\n",
        "\n",
        "    loss.backward() # backwards pass\n",
        "\n",
        "    optimiser.step() #update model parameters\n",
        "\n",
        "    optimiser.zero_grad() #clear gradient\n",
        "\n",
        "    total_loss += loss.item() #track loss\n",
        "\n",
        "\n",
        "  avg_loss = total_loss / num_steps # get average loss\n",
        "  print(avg_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLkvE0YHYtDp",
        "outputId": "c00d39bc-ce3b-488f-a4e2-30ddead3595b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] avg training loss = 0.4301\n",
            "[Epoch 2] avg training loss = 0.2470\n",
            "[Epoch 3] avg training loss = 0.2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation/Dev Set\n",
        "\n",
        "\n",
        "#extracting input ids and attention mask\n",
        "dev_input_ids = tokenised_dev_sentences.input_ids\n",
        "dev_attention_mask = tokenised_dev_sentences.attention_mask\n",
        "\n",
        "#moving data to cuda\n",
        "dev_input_ids = dev_input_ids.to(device)\n",
        "dev_attention_mask = dev_attention_mask.to(device)\n",
        "dev_labels = dev_labels.to(device)\n",
        "\n",
        "\n",
        "N_dev = dev_input_ids.shape[0] #no. of dev examples\n",
        "num_steps_dev = ceil(N_dev / batch_size) #number of batches\n",
        "\n",
        "model.eval() #set model to evaluation\n",
        "total_dev_loss = 0\n",
        "\n",
        "#no gradient tracking\n",
        "with torch.no_grad():\n",
        "  for step in range(num_steps_dev):\n",
        "\n",
        "    low = step * batch_size\n",
        "    high = low + batch_size\n",
        "\n",
        "    batch_input_ids = dev_input_ids[low:high]\n",
        "    batch_attention_mask = dev_attention_mask[low:high]\n",
        "    batch_labels = dev_labels[low:high]\n",
        "\n",
        "    #only forward pass\n",
        "    outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
        "\n",
        "    total_dev_loss += outputs.loss.item()\n",
        "\n",
        "avg_dev_loss = total_dev_loss / num_steps_dev #compute average loss\n",
        "print(avg_dev_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0890ftKncbs4",
        "outputId": "248546eb-ad8b-4b70-a74e-847e7bbadd3b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15620398936993574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATING PREDICTIONS on DEV SET\n",
        "\n",
        "all_preds = [] #emppty list for predicted tag sequences\n",
        "model.eval() #model evaluation mode\n",
        "\n",
        "#disable gradient computing\n",
        "with torch.no_grad():\n",
        "    for step in range(num_steps_dev):\n",
        "        low  = step * batch_size\n",
        "        high = min(low + batch_size, N_dev)\n",
        "\n",
        "        #batch inputs\n",
        "        batch_input_ids = dev_input_ids[low:high]\n",
        "        batch_attention_mask = dev_attention_mask[low:high]\n",
        "\n",
        "        #generate predicted sequences\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=batch_input_ids,\n",
        "            attention_mask=batch_attention_mask,\n",
        "            max_length=dev_sequen_length\n",
        "        )\n",
        "\n",
        "        #decode the generated token IDS into strings and split into tag sequences\n",
        "        pred_strs= tokeniser.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        batch_preds = [s.split() for s in pred_strs]\n",
        "\n",
        "        all_preds.extend(batch_preds) #append batches\n",
        "\n",
        "print(len(all_preds), len(prep_dev_tags))  #test that they should match\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ypNFuykfN0",
        "outputId": "49a6c408-88e1-458c-a7e8-f27da2f4198e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2001 2001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Span Matching Functions\n",
        "\n",
        "\n",
        "#Extract enetity spans from a sequence of BIO tags\n",
        "#returns the start, end and label for each span\n",
        "def get_spans(tags, simplified = False):\n",
        "  spans = []\n",
        "  start = None\n",
        "  label = None\n",
        "\n",
        "  for i, tag in enumerate(tags):\n",
        "    if tag == 'O':\n",
        "      #close the span if there is one\n",
        "      if start is not None:\n",
        "        spans.append((start, i, label))\n",
        "        start = None\n",
        "        label = None\n",
        "\n",
        "    elif tag.startswith('B'):\n",
        "      #if prev span is still open then close it\n",
        "      if start is not None:\n",
        "        spans.append((start, i, label))\n",
        "\n",
        "      start = i\n",
        "      label = '' if simplified else tag[2:] #remove label prefix for simple tagset\n",
        "\n",
        "    elif tag.startswith('I'):\n",
        "      continue #contiue getting the span\n",
        "\n",
        "  if start is not None: #close spans that reach the end of sequence\n",
        "      spans.append((start, len(tags), label))\n",
        "\n",
        "  return spans\n",
        "\n",
        "\n",
        "\n",
        "#Function to change BIO tags by removing the label types (LOC, PER, ORG)\n",
        "def simplify_bio_sequences(seqs):\n",
        "    return [[('O' if t=='O' else t[0]) for t in seq] for seq in seqs]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#calculate the span matching accuracy\n",
        "#span is correct if its start and end and optionally the label match\n",
        "def span_match_score(gold_seqs, pred_seqs):\n",
        "  total = 0 #gold spans\n",
        "  correct = 0 #cirrect pred spans\n",
        "\n",
        "  for gseq, pseq in zip(gold_seqs, pred_seqs):\n",
        "        goldspan = set(get_spans(gseq)) #get gold spans\n",
        "        predspan = set(get_spans(pseq)) #get pred spans\n",
        "        total   += len(goldspan)\n",
        "        correct += len(goldspan & predspan) #count the overlaps\n",
        "\n",
        "  return correct / total if total else 0.0"
      ],
      "metadata": {
        "id": "L_CYAId-mqX2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate span matching accuracy on the dev set\n",
        "\n",
        "#compute labelled span matching score\n",
        "#need to convert the gold tag strings to list but the preds are already a list\n",
        "labelled_acc = span_match_score( [gold.split() for gold in prep_dev_tags],\n",
        "                                   all_preds )\n",
        "\n",
        "#compute unlabelled span matching scores\n",
        "#need to simplify the labels in both the gold and preds\n",
        "unlabelled_acc = span_match_score( simplify_bio_sequences([gold.split() for gold in prep_dev_tags]), simplify_bio_sequences(all_preds) )\n",
        "\n",
        "print(\"Labelled span\", labelled_acc)\n",
        "print(\"Unlabelled span\", unlabelled_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlT9TFGpmyyT",
        "outputId": "f22e7228-8eb3-4d03-f3b4-fdbe1accf9d3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labelled span acc: 0.2484472049689441\n",
            "Unlabelled span acc: 0.2660455486542443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aligning the predicted and gold tag sequences to make sure the evaluation is able to be done\n",
        "\n",
        "\n",
        "def align_preds_to_gold(gold_strs, pred_seqs, pad_tag=\"O\"):\n",
        "    aligned_preds = []\n",
        "    aligned_gold  = []\n",
        "    for gold_s, pred in zip(gold_strs, pred_seqs):\n",
        "        gold_list = gold_s.split()\n",
        "        L = len(gold_list)\n",
        "\n",
        "        # truncate or pad the prediction\n",
        "        if len(pred) >= L:\n",
        "            pred_list = pred[:L] #shorten if too long\n",
        "        else:\n",
        "            pred_list = pred + [pad_tag] * (L - len(pred)) #pad if too short\n",
        "\n",
        "        aligned_gold .append(gold_list)\n",
        "        aligned_preds.append(pred_list)\n",
        "\n",
        "    return aligned_gold, aligned_preds\n"
      ],
      "metadata": {
        "id": "S7JaabzVzNX9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Evaluate classification - full 7 label set\n",
        "\n",
        "#align the gold and pred sequences\n",
        "aligned_gold_seqs, aligned_pred_seqs = align_preds_to_gold(prep_dev_tags, all_preds)\n",
        "\n",
        "# now flatten for the classification report\n",
        "flat_gold = [tag for seq in aligned_gold_seqs for tag in seq]\n",
        "flat_pred = [tag for seq in aligned_pred_seqs for tag in seq]\n",
        "\n",
        "print(len(flat_gold), len(flat_pred))  #check that these match\n",
        "\n",
        "#define tag order for the report\n",
        "label_names = ['O','B-LOC','I-LOC','B-ORG','I-ORG','B-PER','I-PER']\n",
        "\n",
        "#print eval metrics\n",
        "print(classification_report(\n",
        "    flat_gold,\n",
        "    flat_pred,\n",
        "    labels=label_names,\n",
        "    zero_division=0\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0kdBONhzzla",
        "outputId": "e0b88464-3b71-42c0-f1e7-cc82472bb1ca"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25149 25149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.96      0.98      0.97     23653\n",
            "       B-LOC       0.39      0.20      0.26       399\n",
            "       I-LOC       0.18      0.08      0.11       148\n",
            "       B-ORG       0.28      0.06      0.10       224\n",
            "       I-ORG       0.20      0.13      0.16       186\n",
            "       B-PER       0.60      0.55      0.57       343\n",
            "       I-PER       0.46      0.50      0.48       196\n",
            "\n",
            "    accuracy                           0.94     25149\n",
            "   macro avg       0.44      0.36      0.38     25149\n",
            "weighted avg       0.93      0.94      0.93     25149\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST DATA\n",
        "\n",
        "#clone the input IDs so we can modify them and keep originals\n",
        "test_labels = tokenised_test_labels.input_ids.clone()\n",
        "\n",
        "test_size = test_labels.shape[0]\n",
        "test_sequen_length = test_labels.shape[1]\n",
        "\n",
        "#replace the paddin tokens with -100 so theyre ignored in loss\n",
        "for i in range(test_size):\n",
        "  for j in range(test_sequen_length):\n",
        "\n",
        "    if test_labels[i, j].item() == tokeniser.pad_token_id:\n",
        "\n",
        "      test_labels[i, j] =-100\n",
        "\n",
        "#move data to cuda\n",
        "test_input_ids = tokenised_test_sentences.input_ids.to(device)\n",
        "test_attention_mask = tokenised_test_sentences.attention_mask.to(device)\n",
        "test_labels = tokenised_test_labels.to(device)\n",
        "\n",
        "N_test = test_input_ids.size(0)\n",
        "num_steps_test = ceil(N_test / batch_size)\n",
        "\n",
        "all_test_preds = [] #predicted tags\n",
        "model.eval() #puts model in evaluation mode\n",
        "\n",
        "#generating predictions for the test data\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step in range(num_steps_test):\n",
        "        low  = step * batch_size\n",
        "        high = min(low + batch_size, N_test)\n",
        "\n",
        "        #get a batch\n",
        "        batch_input_ids= test_input_ids[low:high]\n",
        "        batch_attention_mask = test_attention_mask[low:high]\n",
        "\n",
        "        #generating the actual output sequences\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=batch_input_ids,\n",
        "            attention_mask=batch_attention_mask,\n",
        "            max_length=test_sequen_length\n",
        "        )\n",
        "\n",
        "        #decode the token ids to strings and then split into tag lists\n",
        "        pred_strs    = tokeniser.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        batch_preds  = [s.split() for s in pred_strs]\n",
        "\n",
        "\n",
        "        all_test_preds.extend(batch_preds)\n",
        "\n",
        "print(len(all_test_preds), len(prep_test_labels))  # they should match\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dagjLd8Pg2AI",
        "outputId": "e4756f89-7f4c-4975-a9a6-342e021d4040"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077 3077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FINAL EVALUATION FROM TEST DATA\n",
        "\n",
        "#align the gold and pred sets\n",
        "aligned_gold_test_seqs, aligned_pred_test_seqs = align_preds_to_gold(prep_test_labels, all_test_preds)\n",
        "\n",
        "# now flatten into single lists for classification report\n",
        "flat_gold_test = [tag for seq in aligned_gold_test_seqs for tag in seq]\n",
        "flat_pred_test = [tag for seq in aligned_pred_test_seqs for tag in seq]\n",
        "\n",
        "#define the label set\n",
        "label_names = ['O','B-LOC','I-LOC','B-ORG','I-ORG','B-PER','I-PER']\n",
        "\n",
        "#print the eval results\n",
        "print(classification_report(\n",
        "    flat_gold_test,\n",
        "    flat_pred_test,\n",
        "    labels=label_names,\n",
        "    zero_division=0\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK4VR58Eifyt",
        "outputId": "9d9b2f86-d10e-46e6-adf9-b598fc1f7b5c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.93      0.97      0.95     43029\n",
            "       B-LOC       0.01      0.00      0.01       742\n",
            "       I-LOC       0.00      0.00      0.00       247\n",
            "       B-ORG       0.05      0.01      0.01       557\n",
            "       I-ORG       0.02      0.01      0.01       431\n",
            "       B-PER       0.08      0.05      0.06       864\n",
            "       I-PER       0.03      0.02      0.03       403\n",
            "\n",
            "   micro avg       0.91      0.91      0.91     46273\n",
            "   macro avg       0.16      0.15      0.15     46273\n",
            "weighted avg       0.87      0.91      0.89     46273\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SPAN ACCURACY FOR TEST SET\n",
        "\n",
        "labelled_acc=span_match_score(aligned_gold_test_seqs, aligned_pred_test_seqs)\n",
        "\n",
        "#simplify the pred and gold sequences first\n",
        "unlabelled_acc = span_match_score(simplify_bio_sequences(aligned_gold_test_seqs), simplify_bio_sequences(aligned_pred_test_seqs))\n",
        "\n",
        "print(\"Labelled span acc:\",   labelled_acc)\n",
        "print(\"Unlabelled span acc:\", unlabelled_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeBHy7yekoXF",
        "outputId": "23587c02-06d3-48c7-c8fb-c88d396297ad"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labelled span acc: 0.0106333795654184\n",
            "Unlabelled span acc: 0.015256588072122053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EVALUATE TEST SET - SIMPLFIED LABEL SET\n",
        "\n",
        "#align the pred and golds\n",
        "aligned_gold, aligned_pred = align_preds_to_gold(prep_test_labels, all_test_preds)\n",
        "\n",
        "#convert the bio tags to the simplified tags\n",
        "gold_simple = [simplify_tags_to_3labels(seq) for seq in aligned_gold]\n",
        "pred_simple = [simplify_tags_to_3labels(seq) for seq in aligned_pred]\n",
        "\n",
        "#flatten the lists for classification report\n",
        "flat_gold_simple = [t for seq in gold_simple for t in seq]\n",
        "flat_pred_simple = [t for seq in pred_simple for t in seq]"
      ],
      "metadata": {
        "id": "fHDvvXXCqZkv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EVALUATE TEST SET - SIMPLFIED LABEL SET\n",
        "\n",
        "#align the pred and golds\n",
        "aligned_gold, aligned_pred = align_preds_to_gold(prep_test_labels, all_test_preds)\n",
        "\n",
        "#convert the bio tags to the simplified tags\n",
        "gold_simple = [simplify_tags_to_3labels(seq) for seq in aligned_gold]\n",
        "pred_simple = [simplify_tags_to_3labels(seq) for seq in aligned_pred]\n",
        "\n",
        "#flatten the lists for classification report\n",
        "flat_gold_simple = [t for seq in gold_simple for t in seq]\n",
        "flat_pred_simple = [t for seq in pred_simple for t in seq]\n",
        "\n",
        "\n",
        "#print the eval metrics\n",
        "print(classification_report(\n",
        "    flat_gold_simple,\n",
        "    flat_pred_simple,\n",
        "    labels=['O', 'B', 'I'],\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "\n",
        "#compute the span level accuracy.\n",
        "labelled_simple_score = span_match_score(gold_simple, pred_simple)\n",
        "unlabelled_simple_score = labelled_simple_score #its the same bc theres no entities\n",
        "\n",
        "print(f\"Simplified labelled span match\", labelled_simple_score:.4f)\n",
        "print(f\"Simplified unlabelled span match\", unlabelled_simple_score:.4f)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpuS2CnWi76N",
        "outputId": "2bc1f49f-f475-4a72-9f98-697cb6ebaf57"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           B       0.09      0.03      0.05      2163\n",
            "           I       0.03      0.02      0.02      1081\n",
            "\n",
            "   micro avg       0.07      0.03      0.04      3244\n",
            "   macro avg       0.04      0.02      0.02      3244\n",
            "weighted avg       0.07      0.03      0.04      3244\n",
            "\n",
            "Simplified F1 scores (B/I/O):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           B       0.09      0.03      0.05      2163\n",
            "           I       0.03      0.02      0.02      1081\n",
            "\n",
            "   micro avg       0.07      0.03      0.04      3244\n",
            "   macro avg       0.04      0.02      0.02      3244\n",
            "weighted avg       0.07      0.03      0.04      3244\n",
            "\n",
            "Simplified labelled span match:   0.0153\n",
            "Simplified unlabelled span match: 0.0153\n"
          ]
        }
      ]
    }
  ]
}