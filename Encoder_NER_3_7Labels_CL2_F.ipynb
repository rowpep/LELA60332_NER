{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from urllib import request\n",
        "import json\n",
        "import pandas as pd\n",
        "from math import floor, ceil, log10\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import random"
      ],
      "metadata": {
        "id": "-d9vFg1PIN8i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set seeds\n",
        "seed = 28\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "E1MizTTgVLGo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_conllu_using_pandas(block):\n",
        "    records = []\n",
        "    for line in block.splitlines():\n",
        "        if not line.startswith('#'):\n",
        "            records.append(line.strip().split('\\t'))\n",
        "    return pd.DataFrame.from_records(\n",
        "        records,\n",
        "        columns=['ID', 'FORM', 'TAG', 'Misc1', 'Misc2'])"
      ],
      "metadata": {
        "id": "6vN6pAj4IPDa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_labels(df):\n",
        "    return (\n",
        "        df.FORM.tolist(),\n",
        "        df.TAG.tolist()\n",
        "    )"
      ],
      "metadata": {
        "id": "ZtsUA9n_IQ8q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREFIX = \"https://raw.githubusercontent.com/UniversalNER/\"\n",
        "DATA_URLS = {\n",
        "    \"en_ewt\": {\n",
        "        \"train\": \"UNER_English-EWT/master/en_ewt-ud-train.iob2\",\n",
        "        \"dev\": \"UNER_English-EWT/master/en_ewt-ud-dev.iob2\",\n",
        "        \"test\": \"UNER_English-EWT/master/en_ewt-ud-test.iob2\"\n",
        "    },\n",
        "    \"en_pud\": {\n",
        "        \"test\": \"UNER_English-PUD/master/en_pud-ud-test.iob2\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "GNe-ZqFtISUH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en_ewt is the main train-dev-test split\n",
        "# en_pud is the OOD test set\n",
        "data_dict = defaultdict(dict)\n",
        "for corpus, split_dict in DATA_URLS.items():\n",
        "    for split, url_suffix in split_dict.items():\n",
        "        url = PREFIX + url_suffix\n",
        "        with request.urlopen(url) as response:\n",
        "            txt = response.read().decode('utf-8')\n",
        "            data_frames = map(parse_conllu_using_pandas,\n",
        "                              txt.strip().split('\\n\\n'))\n",
        "            token_label_alignments = list(map(tokens_to_labels,\n",
        "                                              data_frames))\n",
        "            data_dict[corpus][split] = token_label_alignments\n"
      ],
      "metadata": {
        "id": "B7bbPLtoITes"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the data so that you don't have to redownload it each time.\n",
        "with open('ner_data_dict.json', 'w', encoding='utf-8') as out:\n",
        "    json.dump(data_dict, out, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "Wk-4Qh2_IU5a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each subset of each corpus is a list of tuples where each tuple\n",
        "# is a list of tokens with a corresponding list of labels.\n",
        "\n",
        "# Train on data_dict['en_ewt']['train']; validate on data_dict['en_ewt']['dev']\n",
        "# and test on data_dict['en_ewt']['test'] and data_dict['en_pud']['test']\n",
        "data_dict['en_ewt']['train'][0], data_dict['en_pud']['test'][1]"
      ],
      "metadata": {
        "id": "6fNGLlPFIWEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict['en_ewt']['train']"
      ],
      "metadata": {
        "id": "7Gzdq3FOPpET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting sentences and labels from the data\n",
        "\n",
        "#split training into sentences and labels\n",
        "train_sentences = [x[0] for x in data_dict['en_ewt']['train']]\n",
        "train_labels = [x[1] for x in data_dict['en_ewt']['train']]\n",
        "\n",
        "#split dev set into sentences and labels\n",
        "dev_sentences = [x[0] for x in data_dict['en_ewt']['dev']]\n",
        "dev_labels = [x[1] for x in data_dict['en_ewt']['dev']]\n",
        "\n",
        "#get the test data from both domains\n",
        "test_sentences1 = [x[0] for x in data_dict['en_ewt']['test']]\n",
        "test_labels1 = [x[1] for x in data_dict['en_ewt']['test']]\n",
        "\n",
        "test_sentences2 = [x[0] for x in data_dict['en_pud']['test']]\n",
        "test_labels2 = [x[1] for x in data_dict['en_pud']['test']]\n",
        "\n",
        "#combine the test data\n",
        "test_sentences = test_sentences1 + test_sentences2\n",
        "test_labels = test_labels1 + test_labels2"
      ],
      "metadata": {
        "id": "w1Al3NApMEcw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the label dictionariies for tag conversion\n",
        "\n",
        "#mappijg bio to numneric ids for training\n",
        "label_to_number = {'B-LOC': 1,\n",
        "                   'I-LOC': 2,\n",
        "                   'B-PER': 3,\n",
        "                   'I-PER': 4,\n",
        "                   'B-ORG': 5,\n",
        "                   'I-ORG': 6,\n",
        "                   'O': 0}\n",
        "\n",
        "#reversing the mapping.\n",
        "number_to_label = {1: 'B-LOC',\n",
        "                   2: 'I-LOC',\n",
        "                   3: 'B-PER',\n",
        "                   4: 'I-PER',\n",
        "                   5: 'B-ORG',\n",
        "                   6: 'I-ORG',\n",
        "                   0: '0'}\n"
      ],
      "metadata": {
        "id": "HYUQsGZyMTpq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting tag strings to numeric labels\n",
        "\n",
        "#converting training tag sequences from strings to numeric ids\n",
        "train_label_to_number = []\n",
        "for set_of_labels in train_labels:\n",
        "  new_label = []\n",
        "  for label in set_of_labels:\n",
        "    new_label.append(label_to_number[label])\n",
        "  train_label_to_number.append(new_label)\n",
        "\n",
        "#converting dev tag sequences from strings to numeric ids\n",
        "dev_label_to_number = []\n",
        "for set_of_labels in dev_labels:\n",
        "  new_label = []\n",
        "  for label in set_of_labels:\n",
        "    new_label.append(label_to_number[label])\n",
        "  dev_label_to_number.append(new_label)\n",
        "\n",
        "#converting test tag sequences from strings to numeric ids\n",
        "test_label_to_number = []\n",
        "for set_of_labels in test_labels:\n",
        "  new_label = []\n",
        "  for label in set_of_labels:\n",
        "    new_label.append(label_to_number[label])\n",
        "  test_label_to_number.append(new_label)"
      ],
      "metadata": {
        "id": "-SvtlrKtNvux"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build vocabulary and covert sentences token ids\n",
        "\n",
        "#define pad token\n",
        "pad_token = 0\n",
        "\n",
        "#start index from 1 as 0 is for pad tokens\n",
        "idx = 1\n",
        "\n",
        "#create a word indexed vocabulary from all datasets\n",
        "word_vocabulary = {'<PAD>': pad_token}\n",
        "for set_of_words in train_sentences + test_sentences + dev_sentences:\n",
        "  for word in set_of_words:\n",
        "    if word not in word_vocabulary:\n",
        "      word_vocabulary[word] = idx\n",
        "      idx += 1\n",
        "\n",
        "#converting training sentences to sequences of word indices\n",
        "train_word_to_number = []\n",
        "for set_of_words in train_sentences:\n",
        "  vocab = []\n",
        "  for word in set_of_words:\n",
        "    vocab.append(word_vocabulary[word])\n",
        "  train_word_to_number.append(vocab)\n",
        "\n",
        "#converting dev sentences to word indices and pad token is word is missing\n",
        "dev_word_to_number = []\n",
        "for set_of_words in dev_sentences:\n",
        "  vocab = []\n",
        "  for word in set_of_words:\n",
        "    vocab.append(word_vocabulary.get(word, pad_token))\n",
        "  dev_word_to_number.append(vocab)\n",
        "\n",
        "#same for the test sentences\n",
        "test_word_to_number = []\n",
        "for set_of_words in test_sentences:\n",
        "  vocab = []\n",
        "  for word in set_of_words:\n",
        "    vocab.append(word_vocabulary.get(word, pad_token))\n",
        "  test_word_to_number.append(vocab)"
      ],
      "metadata": {
        "id": "iHdxZpMdYxib"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Names of the different lists for clarity\n",
        "\n",
        "#labels to numbers\n",
        "train_label_to_number\n",
        "dev_label_to_number\n",
        "test_label_to_number\n",
        "\n",
        "#words to numbers\n",
        "train_word_to_number\n",
        "dev_word_to_number\n",
        "test_word_to_number"
      ],
      "metadata": {
        "id": "R2GL6oNsehm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now pad the sentences or cut them short. pad the labels with -100 and the sentences with 0 bc the code needs them all to be the same size and readable.\n",
        "\n",
        "#Setting the max length of the sequences of tokens\n",
        "max_length = 128\n",
        "\n",
        "\n",
        "#PADDING THE LABELS\n",
        "\n",
        "padded_train_labels = []\n",
        "for set_of_labels in train_label_to_number:\n",
        "  if len(set_of_labels) < max_length:\n",
        "    padded_sentence = set_of_labels + [-100] *(max_length - len(set_of_labels))\n",
        "  else:\n",
        "    padded_sentence = set_of_labels[:max_length]\n",
        "  padded_train_labels.append(padded_sentence)\n",
        "\n",
        "\n",
        "padded_dev_labels = []\n",
        "for set_of_labels in dev_label_to_number:\n",
        "  if len(set_of_labels) < max_length:\n",
        "    padded_sentence = set_of_labels + [-100] *(max_length - len(set_of_labels))\n",
        "  else:\n",
        "    padded_sentence = set_of_labels[:max_length]\n",
        "  padded_dev_labels.append(padded_sentence)\n",
        "\n",
        "\n",
        "padded_test_labels = []\n",
        "for set_of_labels in test_label_to_number:\n",
        "  if len(set_of_labels) < max_length:\n",
        "    padded_sentence = set_of_labels + [-100] *(max_length - len(set_of_labels))\n",
        "  else:\n",
        "    padded_sentence = set_of_labels[:max_length]\n",
        "  padded_test_labels.append(padded_sentence)\n",
        "\n",
        "\n",
        "#PADDING THE SENTENCES\n",
        "\n",
        "padded_train_sentences = []\n",
        "for set_of_words in train_word_to_number:\n",
        "  if len(set_of_words) < max_length:\n",
        "    padded_sentence = set_of_words + [0] *(max_length-len(set_of_words))\n",
        "  else:\n",
        "    padded_sentence = set_of_words[:max_length]\n",
        "  padded_train_sentences.append(padded_sentence)\n",
        "\n",
        "padded_dev_sentences = []\n",
        "for set_of_words in dev_word_to_number:\n",
        "  if len(set_of_words) < max_length:\n",
        "    padded_sentence = set_of_words + [0] *(max_length-len(set_of_words))\n",
        "  else:\n",
        "    padded_sentence = set_of_words[:max_length]\n",
        "  padded_dev_sentences.append(padded_sentence)\n",
        "\n",
        "padded_test_sentences = []\n",
        "for set_of_words in test_word_to_number:\n",
        "  if len(set_of_words) < max_length:\n",
        "    padded_sentence = set_of_words + [0] *(max_length-len(set_of_words))\n",
        "  else:\n",
        "    padded_sentence = set_of_words[:max_length]\n",
        "  padded_test_sentences.append(padded_sentence)"
      ],
      "metadata": {
        "id": "MPZBtaAIN0cH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All the padded lists\n",
        "\n",
        "#padded labels\n",
        "\n",
        "padded_train_labels\n",
        "padded_dev_labels\n",
        "padded_test_labels\n",
        "\n",
        "#padded words\n",
        "\n",
        "padded_train_sentences\n",
        "padded_dev_sentences\n",
        "padded_test_sentences"
      ],
      "metadata": {
        "id": "UkiR1y6miuhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reconstruct the raw text from token ids before the training loop\n",
        "\n",
        "\n",
        "label_pad_id = -100 #label for padding\n",
        "token_pad_id = word_vocabulary['<PAD>'] #word padding token id\n",
        "\n",
        "#creating a reversed vocab to map token ids back to the string\n",
        "id2word = {v:k for k,v in word_vocabulary.items()}\n",
        "\n",
        "#getting the raw training sentences excluding padding tokens\n",
        "raw_train_sentences = [\n",
        "    [ id2word[token_id] for token_id in seq\n",
        "      if token_id != token_pad_id ]\n",
        "    for seq in padded_train_sentences\n",
        "]\n",
        "\n",
        "#same for dev sentences\n",
        "raw_dev_sentences = [\n",
        "    [ id2word[token_id] for token_id in seq\n",
        "      if token_id != token_pad_id ]\n",
        "    for seq in padded_dev_sentences\n",
        "]\n",
        "\n",
        "\n",
        "#and padded sentences\n",
        "raw_test_sentences = [\n",
        "    [ id2word[token_id] for token_id in seq\n",
        "      if token_id != token_pad_id ]\n",
        "    for seq in padded_test_sentences\n",
        "]\n"
      ],
      "metadata": {
        "id": "0FlUtWiXyFyc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCODER\n",
        "\n",
        "hidden_dim = 256\n",
        "\n",
        "class SineEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size, #how many tokens is can see\n",
        "                 num_labels, #how many labels to predict 3 or 7\n",
        "                 sequence_length=15, #fixed sentnece lengyh\n",
        "                 d_model=hidden_dim, #how big the embeddings are\n",
        "                 nhead=2, # numvber of attention heads\n",
        "                 n_layers=2, #encoder layers\n",
        "                 dropout=0.1): #transformer layers\n",
        "        super().__init__() #important for pytorch. initialises nn.Module\n",
        "        self.d_model = d_model #saves the dimensions as a property of the model\n",
        "\n",
        "        #load the pretrained BERT model\n",
        "        self.bertmodel = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        #freeze bert weights\n",
        "        for param in self.bertmodel.parameters():\n",
        "          param.requires_grad=False\n",
        "\n",
        "        #creates transformer encoder layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.bertmodel.config.hidden_size, nhead=2, dim_feedforward=hidden_dim, dropout = dropout, batch_first=True)\n",
        "\n",
        "        #stack multiple encoder layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        #dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #linear layer to map embeddings to label logits\n",
        "        self.classifier = nn.Linear(self.bertmodel.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        #input through BERT\n",
        "        outputs = self.bertmodel(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "\n",
        "        #pass through customer transformer encoder layers\n",
        "        contextual_embeddings = self.transformer_encoder(embeddings)\n",
        "\n",
        "        #pass through drop out\n",
        "        contextual_embeddings= self.dropout(contextual_embeddings)\n",
        "\n",
        "        #projct to label space\n",
        "        logits = self.classifier(contextual_embeddings)\n",
        "\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "EYphvIZKIfqy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING LOOP FOR TRANSFORMER\n",
        "\n",
        "\n",
        "def train_transformer_epoch(raw_texts, train_labels, encoder, optimiser, loss_fn, device, batch_size=32):\n",
        "\n",
        "  encoder.train() #model to training mode\n",
        "\n",
        "  n_steps = ceil(len(raw_texts)/batch_size) #batches in one epoch\n",
        "\n",
        "  train_losses = torch.zeros(n_steps) #stores loss\n",
        "\n",
        "  for step_n in tqdm(range(n_steps), leave=False):\n",
        "\n",
        "    #make the batches\n",
        "    low = step_n * batch_size\n",
        "    high = low + batch_size\n",
        "\n",
        "    texts = raw_texts[low:high] #raw text for batch\n",
        "\n",
        "    #tokenise input using BERT tokeniser\n",
        "    encoding = encoder.tokenizer(texts, is_split_into_words=True, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "    #move input to cuda and get corresponding labels\n",
        "    input_ids = encoding.input_ids.to(device)\n",
        "    attention_mask = encoding.attention_mask.to(device)\n",
        "    batch_labels = train_labels[low:high]\n",
        "    batch_labels = torch.tensor(batch_labels).to(device)\n",
        "\n",
        "    #optimser zero grad\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    #forward pass\n",
        "    predictions = encoder(input_ids, attention_mask)\n",
        "\n",
        "    #reshaping stuff so it fits with the loss\n",
        "    predictions = predictions.reshape(-1, predictions.shape[-1])\n",
        "    batch_labels = batch_labels.reshape(-1)\n",
        "\n",
        "    #cross entropy loss\n",
        "    loss = loss_fn(predictions, batch_labels)\n",
        "\n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    #update loss\n",
        "    train_losses[step_n] = loss.item()\n",
        "\n",
        "    #return loss\n",
        "  return train_losses\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1MGqizVKnfUd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VaALIDATION FOR TRAINING LOOP\n",
        "\n",
        "\n",
        "def validate_transformer(raw_texts, dev_labels, encoder, loss_fn, device, batch_size=32):\n",
        "\n",
        "  encoder.eval() #model to evaluation mode\n",
        "\n",
        "  n_steps_dev = ceil(len(raw_texts) / batch_size)\n",
        "  dev_losses = torch.zeros(n_steps_dev) #stores loss\n",
        "\n",
        "  #lists for final outputs\n",
        "  all_predictions = []\n",
        "  all_gold = []\n",
        "\n",
        "  with torch.no_grad(): #disable gradient tracking\n",
        "    for step_n in tqdm(range(n_steps_dev), leave=False, desc='Validation'):\n",
        "\n",
        "      #create batches\n",
        "      low = step_n * batch_size\n",
        "      high = low + batch_size\n",
        "\n",
        "      texts = raw_texts[low:high]\n",
        "\n",
        "      encoding = encoder.tokenizer(texts, is_split_into_words=True, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "      input_ids = encoding.input_ids.to(device)\n",
        "      attention_mask = encoding.attention_mask.to(device)\n",
        "\n",
        "      batch_labels = dev_labels[low:high]\n",
        "      batch_labels = torch.tensor(batch_labels).to(device)\n",
        "\n",
        "      #forward pass\n",
        "      with torch.no_grad():\n",
        "        predictions = encoder(input_ids, attention_mask)\n",
        "\n",
        "      #reshape the data for loss\n",
        "      predictions_flat = predictions.view(-1, predictions.shape[-1])\n",
        "      labels_flat = batch_labels.view(-1)\n",
        "\n",
        "      #loss\n",
        "      loss = loss_fn(predictions_flat, labels_flat)\n",
        "\n",
        "      dev_losses[step_n] = loss.item()\n",
        "\n",
        "      #save the data\n",
        "      preds_indices = predictions.argmax(dim=-1)\n",
        "\n",
        "      for pred_seq, gold_seq in zip(preds_indices, batch_labels):\n",
        "        #convert tensors to numpy arrays\n",
        "        pred_seq = pred_seq.cpu().numpy()\n",
        "        gold_seq = gold_seq.cpu().numpy()\n",
        "\n",
        "        sentence_preds = []\n",
        "        sentence_golds = []\n",
        "\n",
        "        #filter out padded positions\n",
        "        for p, g in zip(pred_seq, gold_seq):\n",
        "          if g!= -100:\n",
        "            sentence_preds.append(p)\n",
        "            sentence_golds.append(g)\n",
        "\n",
        "        all_predictions.append(sentence_preds)\n",
        "        all_gold.append(sentence_golds)\n",
        "\n",
        "        #print examples with named entities to track learning\n",
        "        sp = [int(x) for x in sentence_preds]\n",
        "        sg = [int(x) for x in sentence_golds]\n",
        "        if 1 in sg or 2 in sg or 3 in sg or 4 in sg or 5 in sg or 6 in sg:\n",
        "          print(sp)\n",
        "          print(sg)\n",
        "          print('')\n",
        "\n",
        "    return all_gold, all_predictions, dev_losses\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "18yjDTowvnxD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Span Level Evaluation Functions\n",
        "\n",
        "#Extract entity spans from a sequence of BIO tags\n",
        "#returns the start, end and label for each span\n",
        "def get_spans(tags, simplified = False):\n",
        "  spans = []\n",
        "  start = None\n",
        "  label = None\n",
        "\n",
        "  for i, tag in enumerate(tags):\n",
        "    if tag == '0':\n",
        "      #close span if there is one\n",
        "      if start is not None:\n",
        "        spans.append((start, i, label))\n",
        "        start = None\n",
        "        label = None\n",
        "\n",
        "    elif tag.startswith('B'):\n",
        "       #if prev span is still open then close it\n",
        "      if start is not None:\n",
        "        spans.append((start, i, label))\n",
        "\n",
        "      start = i\n",
        "      label = '' if simplified else tag[2:] #remove label prefix for simple tagset\n",
        "\n",
        "    elif tag.startswith('I'):\n",
        "      continue\n",
        "\n",
        "  if start is not None: #close spans that reach the end of sequence\n",
        "      spans.append((start, len(tags), label))\n",
        "\n",
        "  return spans\n",
        "\n",
        "\n",
        "\n",
        "#Function to change BIO tags by removing the label types (LOC, PER, ORG)\n",
        "def simplify_bio_sequences(seqs):\n",
        "    return [[('0' if t=='O' else t[0]) for t in seq] for seq in seqs]\n",
        "\n",
        "\n",
        "\n",
        "#calculate the span matching accuracy\n",
        "#span is correct if its start and end and optionally the label match\n",
        "def span_match_score(gold_seqs, pred_seqs):\n",
        "  total = 0 #gold spans\n",
        "  correct = 0 #pred spans\n",
        "\n",
        "  for gseq, pseq in zip(gold_seqs, pred_seqs):\n",
        "        goldspans = set(get_spans(gseq)) #get gold spans\n",
        "        predspans = set(get_spans(pseq)) #get pred spans\n",
        "        total   += len(goldspans)\n",
        "        correct += len(goldspans & predspans) #count overlap\n",
        "\n",
        "  return correct / total if total else 0.0"
      ],
      "metadata": {
        "id": "AZPp7i5-WS4J"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING SETUP AND LOOP\n",
        "\n",
        "#use GPU\n",
        "device = 'cuda'\n",
        "\n",
        "#constantvalues for the model\n",
        "vocab_size = len(word_vocabulary)\n",
        "num_labels = 7 #number of bio labels\n",
        "sequence_length = 128\n",
        "\n",
        "#define the encoder model\n",
        "encoder = SineEncoder(\n",
        "    vocab_size=vocab_size,\n",
        "    num_labels=num_labels,\n",
        "    sequence_length=sequence_length,\n",
        "    d_model=256, #hidden dimensions\n",
        "    nhead=2, #number of attention heads\n",
        "    n_layers=2, #transformer layers\n",
        "    dropout=0.1).to(device) # drop out rate\n",
        "\n",
        "#define theoptimiser and weight decay\n",
        "optimiser = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "#define custim weights to look at rare tags more\n",
        "class_weights = torch.tensor([1.0,4.0,4.5,1.0,1.5,1.0,1.0]).to('cuda')\n",
        "\n",
        "#define loss\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100,weight=class_weights)\n",
        "\n",
        "#number of training epochs\n",
        "n_epochs = 30\n",
        "\n",
        "#TRAINING LOOP\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  #trainging the data\n",
        "  train_losses = train_transformer_epoch(raw_train_sentences, padded_train_labels, encoder, optimiser, loss_fn, device, batch_size=32)\n",
        "\n",
        "\n",
        "  #validating the dev set\n",
        "  all_gold, all_predictions, val_losses = validate_transformer(raw_dev_sentences, padded_dev_labels, encoder, loss_fn, device, batch_size=32)\n",
        "\n",
        "  print(\"Train loss\", train_losses.mean())\n",
        "  print(\"Val loss\", val_losses.mean())\n",
        "\n"
      ],
      "metadata": {
        "id": "0Y8vqg8j4Zc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert numeric labelIDS back to string bio tags using the reverse labels dictionary\n",
        "\n",
        "gold_labels = [[number_to_label[i] for i in seq] for seq in all_gold]\n",
        "pred_labels = [[number_to_label[i] for i in seq] for seq in all_predictions]"
      ],
      "metadata": {
        "id": "DzNgutEgjgrF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute Evaluation Metrics\n",
        "\n",
        "#flatten gold and predicted tags into single lists for classification report\n",
        "#each element should be a single tokens label\n",
        "\n",
        "flat_gold = []\n",
        "\n",
        "for sentence in gold_labels:\n",
        "  for tag in sentence:\n",
        "    flat_gold.append(tag)\n",
        "\n",
        "\n",
        "flat_pred = []\n",
        "for sentence in pred_labels:\n",
        "  for tag in sentence:\n",
        "    flat_pred.append(tag)\n",
        "\n",
        "#defining label set for classification report\n",
        "label_names = ['0','B-LOC','I-LOC','B-ORG','I-ORG','B-PER','I-PER']\n",
        "\n",
        "print(classification_report(flat_gold, flat_pred,\n",
        "      labels=label_names, zero_division=0))"
      ],
      "metadata": {
        "id": "jKT1kVXhlMA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for converting full BIO to simplified format\n",
        "\n",
        "def simplify_bio_sequences(seqs):\n",
        "\n",
        "    simple = []\n",
        "    for seq in seqs:\n",
        "        simple_seq = []\n",
        "        for tag in seq:\n",
        "            if tag == '0':\n",
        "                simple_seq.append('O')\n",
        "            else:\n",
        "                simple_seq.append(tag[0]) #only take B or I\n",
        "        simple.append(simple_seq) #append the list of simple tags\n",
        "    return simple\n"
      ],
      "metadata": {
        "id": "Cw23uft6Xmip"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SPAN MATCH ACCURACY\n",
        "\n",
        "#Labelled span match\n",
        "labelled_score = span_match_score(gold_labels, pred_labels)\n",
        "\n",
        "#Unlabelled span match\n",
        "gold_simple = simplify_bio_sequences(gold_labels)\n",
        "pred_simple = simplify_bio_sequences(pred_labels)\n",
        "unlabelled_score = span_match_score(gold_simple, pred_simple)\n",
        "\n",
        "print(f\"Labelled span match\", labelled_score)\n",
        "print(f\"Unlabelled span match\", unlabelled_score)\n"
      ],
      "metadata": {
        "id": "Wn7qCNXQYByq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model on Test Set\n",
        "\n",
        "# Run the model on test data exactly like validation\n",
        "all_gold_test, all_pred_test, test_losses = validate_transformer(\n",
        "    raw_test_sentences,\n",
        "    padded_test_labels,\n",
        "    encoder,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "print(f\"Test loss\", test_losses.mean()) #print average loss\n",
        "\n",
        "# Convert numerical preds to BIO tag strings\n",
        "gold_labels_test = [[ number_to_label[i] for i in seq ]\n",
        "                    for seq in all_gold_test]\n",
        "pred_labels_test = [[ number_to_label[i] for i in seq ]\n",
        "                    for seq in all_pred_test]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Classification Report\n",
        "\n",
        "#flatten the gold and pred lists for the classification report\n",
        "flat_gold = [t for seq in gold_labels_test for t in seq]\n",
        "flat_pred = [t for seq in pred_labels_test for t in seq]\n",
        "\n",
        "label_names = ['0','B-LOC','I-LOC','B-ORG','I-ORG','B-PER','I-PER']\n",
        "\n",
        "print(classification_report(\n",
        "    flat_gold,\n",
        "    flat_pred,\n",
        "    labels=label_names,\n",
        "    zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Span Evaluation\n",
        "\n",
        "#compute labelled span match score\n",
        "labelled_score = span_match_score(gold_labels_test, pred_labels_test)\n",
        "\n",
        "#simplify sequences to simple tagset for unlabelled span match\n",
        "gold_simple    = simplify_bio_sequences(gold_labels_test)\n",
        "pred_simple    = simplify_bio_sequences(pred_labels_test)\n",
        "\n",
        "#compute unlabelled span match score\n",
        "unlabelled_score = span_match_score(gold_simple, pred_simple)\n",
        "\n",
        "print(f\"Labelled span match\", labelled_score)\n",
        "print(f\"Unlabelled span match\", unlabelled_score)\n"
      ],
      "metadata": {
        "id": "iAjN0B9r-pHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simplified Label Set Evaluation\n",
        "\n",
        "#convert full bio sequences to simplified sets for gold and pred lists\n",
        "gold_simple = simplify_bio_sequences(gold_labels_test)\n",
        "pred_simple = simplify_bio_sequences(pred_labels_test)\n",
        "\n",
        "#flatten the lists\n",
        "flat_gold_simple = [t for seq in gold_simple for t in seq]\n",
        "flat_pred_simple = [t for seq in pred_simple for t in seq]\n",
        "\n",
        "print(classification_report(\n",
        "    flat_gold_simple,\n",
        "    flat_pred_simple,\n",
        "    labels=['0', 'B', 'I'],\n",
        "    zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "#compute labelled span match\n",
        "labelled_simple_score = span_match_score(gold_simple, pred_simple)\n",
        "\n",
        "#compute unlabelled span match (same as labelled)\n",
        "unlabelled_simple_score = labelled_simple_score\n",
        "\n",
        "print(f\"Simplified labelled span match\", labelled_simple_score)\n",
        "print(f\"Simplified unlabelled span match\", unlabelled_simple_score)\n",
        "\n"
      ],
      "metadata": {
        "id": "klsP_o3oMuQM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}